{
    "collab_server" : "",
    "contents" : "---\ntitle: \"154 Homework 5\"\nauthor: \"Lindsey Zhang\"\ndate: \"4/9/2017\"\noutput: word_document\n---\n\nNote: My computer can't run bagging/random forests with 6000+ variables so I randomly sampled 1500 variables to run all of my analysis. Anything above 1500 variables takes an extremely long time to run.\n\n##Problem 1 Part 1\nUse CART and sparse logistic regression:\n\n```{r}\nlibrary(spls)\nlibrary(tree)\nset.seed(12)\n\ndata(\"prostate\")\ncancer_y <- prostate$y\nhas_cancer <- ifelse(cancer_y == 1,\"Yes\",\"No\")\ncancer_x <- prostate$x\n\nsample <- sample(6034, 1500)\nsub_cancer_x <- cancer_x[,sample]\n\ncancer <- as.data.frame(cbind(has_cancer, matrix(sub_cancer_x, nrow = 102)))\n\nindx <- sapply(cancer, is.factor)\ncancer[indx] <- lapply(cancer[indx], function(x) as.numeric(as.character(x)))\n\nsub_cancer <- cbind(has_cancer, cancer[,-1])\n\ntrain <- sample(102, 80)\n\ntree_fit <- tree(has_cancer~., data = sub_cancer,  subset = train)\nsummary(tree_fit)\n\nplot(tree_fit)\ntext(tree_fit, pretty = 0)\n\ntree_pred <- predict(tree_fit, sub_cancer[-train, -1], type = \"class\")\ntable(tree_pred, sub_cancer[-train, 1])\n\ncorr_tree_pred <- mean(tree_pred == sub_cancer[-train, 1])\ncorr_tree_pred\n```\n\n```{r}\nlibrary(glmnet)\n\nx_var <- prostate$x\ngenes_res <- prostate$y\n\ngenes <- x_var[,sample]\n\ngene_data <- as.data.frame(cbind(genes_res, genes))\n\nlasso_logistic <- cv.glmnet(genes[train,], genes_res[train], alpha=1, family = \"binomial\")\n\nplot(lasso_logistic)\nlasso_logistic$lambda.min\nunique(as.vector(coef(lasso_logistic)))\n\nlasso_logistic_pred <- predict(lasso_logistic, genes[-train,], type = \"class\", s = lasso_logistic$lambda.min)\n\ntable(lasso_logistic_pred, sub_cancer[-train, 1])\ncorr_logistic_pred <- mean(as.numeric(lasso_logistic_pred) == genes_res[-train])\ncorr_logistic_pred\n```\n\nBoth CART and sparse logistic regression produced the same results. Both predicted 18 our of 22 correct out of the test set. Although results might have turned out differently if more observations were used, the similar results make sense since both method try to pick the most important variables for response prediction. Also, this result tells us that the data is not completely linear because classification trees work better on non-linear data and logistic regression works better on linear data.\n\n##Problem 1 Part 2\nUsing bagging:\n\n```{r}\nlibrary(randomForest)\n\nsub_bag_fit <- randomForest(has_cancer~., data = sub_cancer, mtry = 1500, importance = TRUE, subset = train)\n\nsub_bag_pred <- predict(sub_bag_fit, newdata = sub_cancer[-train,])\ntable(sub_bag_pred, sub_cancer[-train,1])\n\ncorr_bag_pred <- mean(sub_bag_pred == sub_cancer[-train, 1])\ncorr_bag_pred\n```\n\nBagging produced the same correct prediction percentage as the tree method and the sparse logistic regression. Since bagging takes multiple samples from the same training set (bootstrapping), the variance of the predictions are lowered. This lowering of the variance did not make any changes to the prediction accuracy in this case.\n\n##Problem 1 Part 3\nUsing random forests:\nTo find mtry value for this classification fit, use sqrt(variables) which is approximately 39 in this case.\n\n```{r}\nsub_random_fit <- randomForest(has_cancer~., data = sub_cancer, subset = train, mtry = 39, importance = TRUE)\n\nsub_random_pred <- predict(sub_random_fit, sub_cancer[-train,], type = \"class\")\n\ntable(sub_random_pred, sub_cancer[-train,1])\n\ncorr_random_pred <- mean(sub_random_pred == sub_cancer[-train, 1])\ncorr_random_pred\n```\n\nThe random forest method produced better prediction accuracy than the rest of the methods. In this model, 39 random predictors were used at each split of the tree. It seems that since these predictions are less correlated than the bagging predictions, the model resulted in a more accurate outcome.\n\n##Problem 2\n\n```{r}\nset.seed(12)\n\nx1 <- sample(500, 100)\nx2 <- 20:119\nx3 <- runif(100, 1, 200)\n\nb0 <- 12\nb1 <- 1.2\nb2 <- .4\nb3 <- 1.5\nerr <- rnorm(100, 0, 1)\n\ny <- b0 + b1*x1 + b2*x2 + b3*x3 + err\n\nsim <- as.data.frame(cbind(y, x1, x2, x3))\nlinear_train <- sample(100, 80)\n\nlinear_tree_fit <- tree(y~., data = sim, subset = linear_train)\nsummary(linear_tree_fit)\nplot(linear_tree_fit)\ntext(linear_tree_fit, pretty = 0, cex = .7)\n\nlinear_tree_pred <- predict(linear_tree_fit, sim[-train, -1])\n\nmean((linear_tree_pred - sim[-train, 1])^2)\n```\n\n```{r}\nset.seed(12)\nlinear_fit <- lm(y~., data = sim, subset = linear_train)\nsummary(linear_fit)\n\nlinear_pred <- predict(linear_fit, sim[-linear_train, -1])\nmean((linear_pred - sim[-linear_train, 1])^2)\n```\n\nThe mean squared error from the decision tree was 2051.6. while the MSE from linear regression was 1.159051. Clearly, the decision tree is not a good method for very linear data since the tree fit was so bad.\n\n##Problem 3 (Tree)\n\n```{r}\nset.seed(12)\n\nx1 <- sample(500, 100)\nx2 <- 20:119\nx3 <- runif(100, 1, 200)\nx4 <- sample (200, 100)\nx5 <- runif(100, 100, 300)\n\nb0 <- 12\nb1 <- 1.2\nb2 <- .4\nb3 <- 1.5\nb4 <- 2.1\nb5 <- .8\nerr <- rnorm(x2, 0, 1)\n\ny <- b0 + b1*x1 + b2*x2 + b3*x3 + b4*x4 + b5*x5 + err\n\nsim2 <- as.data.frame(cbind(y,x1,x2,x3,x4,x5))\n\ntree_fit_0 <- tree(y~., data = sim2, subset = linear_train)\nsummary(tree_fit_0)\nplot(tree_fit_0)\ntext(tree_fit_0, pretty = 0, cex = .7)\n\ntree_pred_0 <- predict(tree_fit_0, sim2[-linear_train, -1])\n\nmean((tree_pred_0 - sim2[-linear_train, 1])^2)\n```\n\n5 useless predictors:\n\n```{r}\nset.seed(12)\n\n#for loop to generate 5 columns of useless predictors\nsamp <- c(sample(50, 5))\npred_5 <- numeric()\n\nfor(k in samp){\n  gaus <- rnorm(100, k, 1)\n  pred_5 <- cbind(pred_5, gaus)\n}\n\nsim5 <- as.data.frame(cbind(sim2, pred_5))\ncolnames(sim5) <- paste(\"x\", c(\"y\", 1:10), sep = \"_\")\n\n\ntree_fit_5 <- tree(x_y~., data = sim5, subset = linear_train)\nsummary(tree_fit_5)\nplot(tree_fit_5)\ntext(tree_fit_5, pretty = 0, cex = 0.7)\n\ntree_pred_5 <- predict(tree_fit_5, sim5[-linear_train, -1])\n\nmean((tree_pred_5 - sim5[-linear_train, 1])^2)\n```\n\n10 useless predictors:\n\n```{r}\nsamp <- sample(50, 10)\npred_10 <- numeric()\n\nfor(k in samp){\n  gaus <- rnorm(100, k, 1)\n  pred_10 <- cbind(pred_10, gaus)\n}\n\nsim10 <- as.data.frame(cbind(sim2, pred_10))\ncolnames(sim10) <- paste(\"x\", c(\"y\", 1:15), sep = \"_\")\n\n\ntree_fit_10 <- tree(x_y~., data = sim10, subset = linear_train)\nsummary(tree_fit_10)\nplot(tree_fit_10)\ntext(tree_fit_10, pretty = 0, cex = 0.7)\n\ntree_pred_10 <- predict(tree_fit_10, sim10[-linear_train, -1])\n\nmean((tree_pred_10 - sim10[-linear_train, 1])^2)\n```\n\n30 useless predictors:\n\n```{r}\nset.seed(12)\n\nsamp2 <- sample(50, 30)\npred_30 <- numeric()\n\nfor (k in samp2){\n  gaus <- rnorm(100, k, 1)\n  pred_30 <- cbind(pred_30, gaus)\n}\n\nsim30 <- as.data.frame(cbind(sim2, pred_30))\ncolnames(sim30) <- paste(\"x\", c(\"y\", 1:35), sep = \"_\")\n\ntree_fit_30 <- tree(x_y~., data = sim30, subset = linear_train)\nsummary(tree_fit_30)\nplot(tree_fit_30)\ntext(tree_fit_30, pretty = 0, cex = .5)\n\ntree_pred_30 <- predict(tree_fit_30, sim30[-linear_train, -1])\n\nmean((tree_pred_30 - sim30[-linear_train, 1])^2)\n```\nWhat is the impact on your CART tree? Is CART able to pick the “right” variables? How does prediction error change when you add more predictors?\n\n0 useless pred: 21068.59\n5 useless pred: 21008.28\n10 useless pred: 25350.21\n30 useless pred: 25139.44\n\nAs the useless predictors increased, the trees gained branches. This is because the model believes some useless predictors are involved in predictions. With 5 useless predictors, the CART tree was able to pick mostly important predictors, except for one.  With 10 useless predictors, the CART tree picked mostly important predictors, except for one. With 30 useless predictors, the CART tree chose 6 useless predictors. Clearly, it became harder for the CART tree to pick out the important variables as they increased and therefore the MSE increased.\n\n##Problem 3 (Random Forests)\n\n```{r}\nset.seed(12)\n\nrandom_fit_0 <- randomForest(y~., data = sim, subset = linear_train, mtry = 2, importance = TRUE)\n\nrandom_pred_0 <- predict(random_fit_0, sim[-linear_train,])\n\nmean((random_pred_0 - sim[-linear_train, 1])^2)\n```\n\n```{r}\nset.seed(12)\n\nrandom_fit_5 <- randomForest(x_y~., data = sim5, subset = linear_train, mtry = 3, importance = TRUE)\n\nrandom_pred_5 <- predict(random_fit_5, sim5[-linear_train,])\n\nmean((random_pred_5 - sim5[-linear_train, 1])^2)\n```\n\n```{r}\nset.seed(12)\n\nrandom_fit_10 <- randomForest(x_y~., data = sim10, subset = linear_train, mtry = 4, importance = TRUE)\n\nrandom_pred_10 <- predict(random_fit_10, sim10[-linear_train,])\n\nmean((random_pred_10 - sim10[-linear_train, 1])^2)\n```\n\n```{r}\nset.seed(12)\n\nrandom_fit_30 <- randomForest(x_y~., data = sim30, subset = linear_train, mtry = 6, importance = TRUE)\n\nrandom_pred_30 <- predict(random_fit_30, sim30[-linear_train,])\n\nmean((random_pred_30 - sim30[-linear_train, 1])^2)\n```\n\nAs the useless predictors increase, the MSE becomes larger and larger. This makes sense for random forests because at each point in the tree, sqrt(# variables) is randomly chosen out of all variables to be tested. This method gives all of the variables a chance. However, since the number of useless predictors outnumber the useful predictors by a lot, this method doesn't work well with this data set.\n\n##Problem 4\n\nThe piecewise linear data could be modeled by fitting a different linear regression for the response in each leaf node using the data points in that leaf. This is different from the piecewise constant model because the piecewise constant model just averages all the data in a node into a constant response.\n\nPseudo code:\nSplit data into training and testing.\nUse CART on the training data.\nGroup the observations of each leaf node together.\nUse linear regression for each leaf node's observations.\nUse CART on the testing data.\nPredict the response by plugging each observation into its corresponding linear equation based on the leaf node.\n\nExplanation:\nInstead of giving each observation that ends up at a certain leaf node the same value, I would use linear regression on the points gathered at each of the leaf nodes. I could then predict the test response based on the linear equation created at the leaf node that it ends up. This could give a slightly more accurate answer than just the average of all points at the leaf node.\n\n##Problem 5\n\n```{r}\nlibrary(gbm)\nset.seed(12)\n\nhas_cancer_num <- ifelse(sub_cancer[,1] == \"Yes\",1,0)\n\nsub_cancer2 <- cbind(has_cancer_num, sub_cancer[,-1])\n\nboost_fit <- gbm(has_cancer_num~., data = sub_cancer2[train,], distribution = \"adaboost\", n.trees = 1000, interaction.depth = 1, shrinkage = 0.01)\n\nsummary(boost_fit)\n\npar(mfrow=c(1,2)) \nplot(boost_fit ,i=\"V749\") \nplot(boost_fit ,i=\"V212\")\n\nboost_pred <- predict(boost_fit, newdata = sub_cancer2[-train,], n.trees=1000, type = \"response\")\ntable(boost_pred, sub_cancer2[-train,1])\n```\n\ncorrect: 20/22 = 0.909\n\nBy fitting the tree to current residuals instead of the outcome, this model was able to perform as well as the random forests method. By taking into account the trees that have already been created, this model should likely outperform random forests.\n\n\n\n\n",
    "created" : 1493772855233.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "964795522",
    "id" : "3BCA9380",
    "lastKnownWriteTime" : 1491975023,
    "last_content_update" : 1491975023,
    "path" : "~/Desktop/154/154 Homework 5.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 9,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}